{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682de96c",
   "metadata": {},
   "source": [
    "# EncouRAGe Tutorial\n",
    "\n",
    "Steps:\n",
    "1. Load data with Hugging Face\n",
    "2. Put context into `Document`s\n",
    "3. Init BatchInferenceRunner and Template\n",
    "4. Select and initialize RAG method\n",
    "5. Run two inference examples (plain + structured)\n",
    "6. Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "HF_HOME = str((Path.cwd() / '.cache' / 'huggingface').resolve())\n",
    "os.environ['HF_HOME'] = HF_HOME\n",
    "os.environ['VLLM_API_KEY'] = 'token-abc123'\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from encourage.llm import BatchInferenceRunner, SamplingParams\n",
    "from encourage.prompts import Context, Document, MetaData\n",
    "from encourage.rag import BaseRAG, BaseRAGConfig, HydeRAGConfig, HydeRAG\n",
    "from encourage.metrics import map_pydantic_field_to_response, F1, NumberMatch, MeanReciprocalRank, RecallAtK, AnswerFaithfulness, ExactMatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76917c9e",
   "metadata": {},
   "source": [
    "## 1) Load Data with HF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hf = load_dataset(\"G4KMU/hotpot_qa\", split=\"validation[:20]\").to_pandas()  # ty:ignore[unresolved-attribute]\n",
    "dataset_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c386f",
   "metadata": {},
   "source": [
    "## 2) Put Context into Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the user prompts\n",
    "user_prompts = dataset_hf['question'].tolist()  # ty:ignore[not-subscriptable]\n",
    "\n",
    "\n",
    "## Create the context collection\n",
    "context_collection = []\n",
    "for _, row in dataset_hf.iterrows():  # ty:ignore[unresolved-attribute]\n",
    "    context_collection.append(\n",
    "        Document(\n",
    "            id=uuid.uuid5(uuid.NAMESPACE_DNS, str(row.get('context_id', ''))),\n",
    "            content=row.get('context', ''),\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "## Create the meta data collection\n",
    "meta_datas = []\n",
    "for idx, row in dataset_hf.iterrows():  # ty:ignore[unresolved-attribute]\n",
    "    reference_answer = row.get('answer')\n",
    "    meta_datas.append(\n",
    "        MetaData(\n",
    "            {\n",
    "                'id': str(row.get('id', idx)),\n",
    "                'reference_answer': reference_answer,\n",
    "                'reference_document': context_collection[idx],  # ty:ignore[invalid-argument-type]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(len(user_prompts))\n",
    "print(len(context_collection))\n",
    "print(len(meta_datas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab2891",
   "metadata": {},
   "source": [
    "## 3) Init BatchInferenceRunner and Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094024d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0, max_tokens=3000)\n",
    "runner = BatchInferenceRunner(sampling_params, \"meta-llama/Meta-Llama-3.1-8B-Instruct\", base_url=\"http://localhost:18124/v1/\")\n",
    "template_name = \"hotpotqa_template.j2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2b583",
   "metadata": {},
   "source": [
    "## 4) Select RAG Method and Initialize it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad14bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_config_1 = BaseRAGConfig(\n",
    "    context_collection=context_collection,\n",
    "    collection_name=\"hotpotqa_test\",\n",
    "    embedding_function='intfloat/multilingual-e5-large-instruct',\n",
    "    top_k=5,\n",
    "    runner=runner,\n",
    "    template_name=template_name,\n",
    "    retrieval_only=False,\n",
    ")\n",
    "\n",
    "rag_method_instance = BaseRAG(rag_config_1)\n",
    "\n",
    "rag_config_2 = HydeRAGConfig(\n",
    "    context_collection=context_collection,\n",
    "    collection_name=\"hotpotqa_test\",\n",
    "    embedding_function='intfloat/multilingual-e5-large-instruct',\n",
    "    top_k=5,\n",
    "    runner=runner,\n",
    "    template_name=template_name,\n",
    "    retrieval_only=False,\n",
    "    additional_prompt=\"Please write a passage to answer the question:\"\n",
    ")\n",
    "\n",
    "rag_method_instance_2 = HydeRAG(rag_config_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850c46cd",
   "metadata": {},
   "source": [
    "## 5) Two inference examples\n",
    "\n",
    "- Example A: without structured output\n",
    "- Example B: with structured output via a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eac601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example A: plain output (no response_format)\n",
    "sys_prompt = Path('/ltstorage/home/strich/encourage/docs/demo/hotpotqa.txt').read_text(encoding='utf-8')\n",
    "\n",
    "responses_plain = rag_method_instance.run(\n",
    "    runner=runner,\n",
    "    sys_prompt=sys_prompt,\n",
    "    user_prompts=user_prompts,\n",
    "    meta_datas=meta_datas,\n",
    "    retrieval_queries=user_prompts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_plain.print_response_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af36bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLI_SYS_PROMPT = (\n",
    "    \"Create verdicts for each statement based on the context. A verdict of 1 means the \"\n",
    "    \"statement is supported by the context, while a verdict of 0 means it is not \"\n",
    "    \"supported. Just return one output per statement, in the same order as the statements.\"\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    AnswerFaithfulness(runner, nli_sys_prompt=NLI_SYS_PROMPT),\n",
    "    F1(),\n",
    "    MeanReciprocalRank(),\n",
    "    RecallAtK(k=3),\n",
    "    RecallAtK(k=5),\n",
    "]\n",
    "\n",
    "\n",
    "for metric in metrics:\n",
    "    result = metric(responses_plain)\n",
    "    print(f\"{metric.name}: {result.score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example B: structured output with an explicit class\n",
    "sys_prompt = Path('/ltstorage/home/strich/encourage/docs/demo/hotpotqa_structured.txt').read_text(encoding='utf-8')\n",
    "\n",
    "class HotPotQAResponse(BaseModel):\n",
    "    reasoning_steps: list[str]\n",
    "    list_of_supporting_facts: list[str]\n",
    "    final_answer: str\n",
    "\n",
    "responses = rag_method_instance.run(\n",
    "    runner=runner,\n",
    "    sys_prompt=sys_prompt,\n",
    "    user_prompts=user_prompts,\n",
    "    meta_datas=meta_datas,\n",
    "    retrieval_queries=user_prompts,\n",
    "    response_format=HotPotQAResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac569bcb",
   "metadata": {},
   "source": [
    "## 6) Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bc85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    F1(),\n",
    "    ExactMatch(),\n",
    "    MeanReciprocalRank(),\n",
    "    RecallAtK(k=3),\n",
    "    RecallAtK(k=5)\n",
    "]\n",
    "responses_new = map_pydantic_field_to_response(responses, HotPotQAResponse, \"final_answer\")\n",
    "for metric in metrics:\n",
    "    result = metric(responses_new)\n",
    "    print(f\"{metric.name}: {result.score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encourage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
