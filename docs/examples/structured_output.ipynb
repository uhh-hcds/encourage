{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from encourage.llm.inference_runner import BatchInferenceRunner\n",
    "from encourage.prompts.prompt_collection import PromptCollection\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-29 15:38:11 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 10-29 15:38:11 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 10-29 15:38:11 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-29 15:38:15 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 10-29 15:38:16 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aef30916ede4641944456f82cc6f4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-29 15:38:21 model_runner.py:1025] Loading model weights took 14.9888 GB\n",
      "INFO 10-29 15:38:23 gpu_executor.py:122] # GPU blocks: 14673, # CPU blocks: 2048\n",
      "INFO 10-29 15:38:27 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-29 15:38:27 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-29 15:38:53 model_runner.py:1456] Graph capturing finished in 27 secs.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "llm = LLM(model=model_name, gpu_memory_utilization=0.95)\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the toy data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompts = \"You are a helpful AI.\"\n",
    "\n",
    "# List of user prompts (questions or requests for the AI)\n",
    "user_prompts = [\"What is the capital of France?\", \"What is the capital of Germany?\"] * 5\n",
    "\n",
    "# Context information for each prompt (additional data or background info)\n",
    "contexts = [{\"key1\": \"value1\"}, {\"key2\": \"value2\"}] * 5\n",
    "\n",
    "# Metadata associated with each prompt (e.g., priority, tags)\n",
    "meta_datas = [{\"meta\": \"data1\"}, {\"meta\": \"data2\"}] * 5\n",
    "\n",
    "# Create a PromptCollection using the create_prompts method\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts,  # System prompt or list of system prompts\n",
    "    user_prompts=user_prompts,  # List of user prompts\n",
    "    contexts=contexts,  # List of context dictionaries (optional)\n",
    "    meta_datas=meta_datas,  # List of metadata dictionaries (optional)\n",
    "    model_name=model_name,  # The name of the model being used (optional)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the Inference Runner with no structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the user request and add a structured output with pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.37it/s, est. speed input: 184.21 toks/s, output: 151.05 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "ðŸ“š Added Context keys: key1 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='John' age=30 id='user_001'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data1'}\n",
      "ðŸ†” Request ID: b4541705-4e0d-40bb-a9ad-f75a313e33f4\n",
      "ðŸ†” Prompt ID: f4668b41-a599-4175-9787-fa5261e2ef9d\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "ðŸ“š Added Context keys: key2 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='Emily' age=28 id='user_1'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data2'}\n",
      "ðŸ†” Request ID: a653a049-77fc-40f0-a359-9d60cbd3a086\n",
      "ðŸ†” Prompt ID: f24e1f2a-bbbf-4e7b-b1a9-8e406bc0c797\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "ðŸ“š Added Context keys: key1 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='John' age=30 id='user_1'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data1'}\n",
      "ðŸ†” Request ID: 5ea0b507-1cb4-4afc-88f9-4840ddaeafc0\n",
      "ðŸ†” Prompt ID: c54dcfd6-eb32-4c89-b21e-39a7f738a9b4\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "ðŸ“š Added Context keys: key2 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='Emily' age=28 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data2'}\n",
      "ðŸ†” Request ID: 4ebd9ae3-91a0-4b9c-aded-9a969c081018\n",
      "ðŸ†” Prompt ID: 7bac0c54-6de2-4e9a-911a-642579b6098c\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "ðŸ“š Added Context keys: key1 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='John Doe' age=32 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data1'}\n",
      "ðŸ†” Request ID: c12e4083-c993-4870-8f73-3ddf23223107\n",
      "ðŸ†” Prompt ID: fe9a93fc-437f-4424-80cd-011300e196db\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "ðŸ“š Added Context keys: key2 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='Emily' age=25 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data2'}\n",
      "ðŸ†” Request ID: 464fb508-d97f-4856-8966-bab63f170f51\n",
      "ðŸ†” Prompt ID: 364058bf-c004-4693-9dfc-fd971df2d274\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "ðŸ“š Added Context keys: key1 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='John' age=30 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data1'}\n",
      "ðŸ†” Request ID: 27c5786f-7b25-49bc-bd44-4167a1e8e90e\n",
      "ðŸ†” Prompt ID: 55057d46-8d33-4cd1-8a85-5ac54ab662cf\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "ðŸ“š Added Context keys: key2 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='Emily' age=28 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data2'}\n",
      "ðŸ†” Request ID: e703a375-f08f-4965-b60c-800b66b3fcd0\n",
      "ðŸ†” Prompt ID: e174649b-756a-4993-a951-933e00a1dd45\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "ðŸ“š Added Context keys: key1 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='John' age=30 id='user_001'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data1'}\n",
      "ðŸ†” Request ID: f905f6a0-20a6-4a50-bfd0-d5968272eabd\n",
      "ðŸ†” Prompt ID: 3199b1c6-f05d-4367-9e35-ea39f91b3497\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "ðŸ“š Added Context keys: key2 (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "name='Sarah' age=28 id='user123'\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'meta': 'data2'}\n",
      "ðŸ†” Request ID: 4a5f3254-9e86-43a4-9252-23a433f3ccc9\n",
      "ðŸ†” Prompt ID: 35435b7a-cfb0-4387-9ac8-2e4d4f1d46d4\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.0 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_prompts = [\"Return a male User\", \"Return a female User\" ] * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas, \n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    id: str\n",
    "    \n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection, schema=User)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the user request and add a structured output with custom json model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_prompts = [\"Return a male User\", \"Return a female User\" ] * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas, \n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "schema = \"\"\"\n",
    "{\n",
    "  \"title\": \"User\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"name\": {\"type\": \"string\"},\n",
    "    \"last_name\": {\"type\": \"string\"},\n",
    "    \"id\": {\"type\": \"integer\"},\n",
    "    \"height\": {\"type\": \"integer\"}\n",
    "  },\n",
    "  \"required\": [\"name\", \"last_name\", \"id\", \"height\"]\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection, schema=schema)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the user request and add a structured output with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_prompts = [\"Return two integers named a and b respectively between 10-100. a is odd and b even.\", \"Return two integers named a and b respectively. a is odd and b even.\"]  * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas, \n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "def add(a: int, b: int):\n",
    "    return a + b\n",
    "    \n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection, schema=schema)\n",
    "responses.print_response_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
