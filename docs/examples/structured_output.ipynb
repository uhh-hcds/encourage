{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "from encourage.llm import BatchInferenceRunner\n",
    "from encourage.prompts.prompt_collection import PromptCollection\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from encourage.prompts.context import Context\n",
    "from encourage.prompts.meta_data import MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init vllm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 09:49:01 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 12-06 09:49:01 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-06 09:49:01 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-06 09:49:01 llm_engine.py:249] Initializing an LLM engine (v0.6.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 12-06 09:49:02 selector.py:135] Using Flash Attention backend.\n",
      "INFO 12-06 09:49:05 model_runner.py:1072] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 12-06 09:49:06 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5799c8f3eba427799b9fc63efa6d04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 09:49:10 model_runner.py:1077] Loading model weights took 14.9888 GB\n",
      "INFO 12-06 09:49:12 worker.py:232] Memory profiling results: total_gpu_memory=47.32GiB initial_memory_usage=15.31GiB peak_torch_memory=16.16GiB memory_usage_post_profile=15.35Gib non_torch_memory=0.35GiB kv_cache_size=28.44GiB gpu_memory_utilization=0.95\n",
      "INFO 12-06 09:49:12 gpu_executor.py:113] # GPU blocks: 14560, # CPU blocks: 2048\n",
      "INFO 12-06 09:49:12 gpu_executor.py:117] Maximum concurrency for 131072 tokens per request: 1.78x\n",
      "INFO 12-06 09:49:16 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-06 09:49:16 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-06 09:49:43 model_runner.py:1518] Graph capturing finished in 27 secs, took 0.26 GiB\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "llm = LLM(model=model_name, gpu_memory_utilization=0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the toy data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompts = \"You are a helpful AI.\"\n",
    "\n",
    "# List of user prompts (questions or requests for the AI)\n",
    "user_prompts = [\"What is the capital of France?\", \"What is the capital of Germany?\"] * 5\n",
    "\n",
    "# # Context information for each prompt (additional data or background info)\n",
    "contexts = [Context.from_prompt_vars({\"key1\": \"value1\"}), Context.from_prompt_vars({\"key2\": \"value2\"})] * 5\n",
    "\n",
    "# # Metadata associated with each prompt (e.g., priority, tags)\n",
    "meta_datas = [MetaData({\"meta\": \"data1\"}), MetaData({\"meta\": \"data2\"})] * 5\n",
    "\n",
    "# Create a PromptCollection using the create_prompts method\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts,  # System prompt or list of system prompts\n",
    "    user_prompts=user_prompts,  # List of user prompts\n",
    "    model_name=model_name,  # The name of the model being used (optional)\n",
    "    contexts=contexts,  # List of Context objects\n",
    "    meta_datas=meta_datas,  # List of MetaData objects\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init the Inference Runner with no structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 26.03it/s, est. speed input: 729.11 toks/s, output: 234.35 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of France is Paris.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 40\n",
      "ðŸ†” Prompt ID: 0448c5af-e644-4d77-a62e-c28ed1bf2888\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3626 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of Germany?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 41\n",
      "ðŸ†” Prompt ID: 24c15e3c-22cc-47a6-803c-0594cca9cf6e\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3617 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of France is Paris.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 42\n",
      "ðŸ†” Prompt ID: f0e5c24c-65a7-48e8-94a4-5f859380e856\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3615 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of Germany?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 43\n",
      "ðŸ†” Prompt ID: f5bf1ffe-1834-4f8f-97e9-38e860eb3ab6\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3614 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of France is Paris.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 44\n",
      "ðŸ†” Prompt ID: ef8bf261-161e-48b5-bbd8-b2065e989eb8\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3612 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of Germany?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 45\n",
      "ðŸ†” Prompt ID: 1193f026-729b-4d67-aee1-c3f411f76670\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.361 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of France is Paris.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 46\n",
      "ðŸ†” Prompt ID: ea10540c-7fcf-42d6-bc14-d40072cf7d61\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3609 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of Germany?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 47\n",
      "ðŸ†” Prompt ID: 04b34249-2ea2-47dc-aacc-cf7fb5d62567\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3607 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of France?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of France is Paris.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 48\n",
      "ðŸ†” Prompt ID: c9614f6d-d77f-4321-a036-2b5a1c007f9b\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3606 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the capital of Germany?\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The capital of Germany is Berlin.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 49\n",
      "ðŸ†” Prompt ID: 24d1e1a9-d5f8-4341-aef8-9292242281c2\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.3604 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=100)\n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the user request and add a structured output with pydantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 11.49it/s, est. speed input: 287.30 toks/s, output: 235.58 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"age\": 30, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 50\n",
      "ðŸ†” Prompt ID: 72f7a9a6-b5be-4c2c-94f1-65f2961d970c\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.9543 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Sarah\", \"age\": 32, \"id\": \"user1\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 51\n",
      "ðŸ†” Prompt ID: 72f512db-cfa1-4c06-b607-3a5c5b937640\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8228 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"age\": 30, \"id\": \"user1\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 52\n",
      "ðŸ†” Prompt ID: 2583f033-6858-41dd-a5b2-196f9ddf7dbf\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8223 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Sarah\", \"age\": 30, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 53\n",
      "ðŸ†” Prompt ID: f1744f23-5a45-422c-81ea-6743c43d7d69\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8221 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"age\": 30, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 54\n",
      "ðŸ†” Prompt ID: 583ffb2a-4078-43ae-9616-2135b69c9283\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8219 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Sarah\", \"age\": 30, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 55\n",
      "ðŸ†” Prompt ID: 92c5e23f-06cb-4ffc-98c0-5192c589ccea\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8218 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John Doe\", \"age\": 30, \"id\": \"12345\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 56\n",
      "ðŸ†” Prompt ID: b5dacc60-917d-4589-a5e0-c6b0ae4d6504\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8493 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Emily\", \"age\": 25, \"id\": \"Our-12345\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 57\n",
      "ðŸ†” Prompt ID: e2ebb1f9-87b1-4df5-82ab-059765d610be\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.874 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"age\": 32, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 58\n",
      "ðŸ†” Prompt ID: 0f8dbd9c-43ba-43b5-a6e5-a913ce6ca15a\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8213 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Samantha\", \"age\": 28, \"id\": \"user123\"}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 59\n",
      "ðŸ†” Prompt ID: 7cf51ac1-35db-4878-b5a6-789c20197105\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.8735 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompts = [\"Return a male User\", \"Return a female User\" ] * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    model_name=model_name,\n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas\n",
    ")\n",
    "\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    id: str\n",
    "    \n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)\n",
    "sampling_params.guided_decoding = GuidedDecodingParams(json=User.model_json_schema())\n",
    "\n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the user request and add a structured output with custom json model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 38.40it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  9.29it/s, est. speed input: 232.33 toks/s, output: 255.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"last_name\": \"Doe\", \"id\": 12345, \"height\": 180}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 60\n",
      "ðŸ†” Prompt ID: 93bcac05-afb4-4bbc-afc0-819d7363d7f0\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 4.4886 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Emily\", \"last_name\": \"Johnson\", \"id\": 12345, \"height\": 5}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 61\n",
      "ðŸ†” Prompt ID: dfc6e411-e7d1-42d2-915f-8a1113686e65\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0281 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"last_name\": \"Doe\", \"id\": 1, \"height\": 180}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 62\n",
      "ðŸ†” Prompt ID: 64576fc0-8e45-4967-894a-db63f14fb540\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0275 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Emily Wilson\", \"last_name\": \"Wilson\", \"id\": 1234, \"height\": 165}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 63\n",
      "ðŸ†” Prompt ID: 89cb6489-e15b-43b7-890c-05fdb6031f50\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0533 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"last_name\": \"Doe\", \"id\": 12345, \"height\": 180}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 64\n",
      "ðŸ†” Prompt ID: 3ce9204f-47cc-4d03-a349-32caf50aa49a\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0776 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Emily\", \"last_name\": \"Johnson\", \"id\": 12345, \"height\": 165}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 65\n",
      "ðŸ†” Prompt ID: d8eb9d66-81c8-4f72-88a7-f2942e6e09ab\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0269 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John\", \"last_name\": \"Doe\", \"id\": 12345, \"height\": 180}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 66\n",
      "ðŸ†” Prompt ID: 080862d8-5e1c-46a7-9899-c601f5ef0b38\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0528 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Emily\", \"last_name\": \"Johnson\", \"id\": 12345, \"height\": 165}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 67\n",
      "ðŸ†” Prompt ID: 38abf49e-5f46-4301-a1e1-22b8a1575638\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0266 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a male User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"John Smith\", \"last_name\": \"Smith\", \"id\": 1234, \"height\": 180}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data1'})\n",
      "ðŸ†” Request ID: 68\n",
      "ðŸ†” Prompt ID: 7d9e853d-1ef4-4132-afcc-59dd5c630543\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0525 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Return a female User\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "{\"name\": \"Sarah\", \"last_name\": \"Johnson\", \"id\": 1, \"height\": 170}\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "You are a helpful AI.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: MetaData(tags={'meta': 'data2'})\n",
      "ðŸ†” Request ID: 69\n",
      "ðŸ†” Prompt ID: 24911ed7-5881-458b-ab23-8103687c2c9e\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 0.9973 seconds\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_prompts = [\"Return a male User\", \"Return a female User\" ] * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas, \n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "schema = \"\"\"\n",
    "{\n",
    "  \"title\": \"User\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"name\": {\"type\": \"string\"},\n",
    "    \"last_name\": {\"type\": \"string\"},\n",
    "    \"id\": {\"type\": \"integer\"},\n",
    "    \"height\": {\"type\": \"integer\"}\n",
    "  },\n",
    "  \"required\": [\"name\", \"last_name\", \"id\", \"height\"]\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "guided_decoding_params = GuidedDecodingParams(json=schema)\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)\n",
    "sampling_params.guided_decoding = guided_decoding_params\n",
    "\n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also add a structured output to a inference runner that is already initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\"Return a male User\", \"Return a female User\" ] * 5\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts, \n",
    "    user_prompts=user_prompts, \n",
    "    contexts=contexts, \n",
    "    meta_datas=meta_datas, \n",
    "    model_name=model_name\n",
    ")\n",
    "\n",
    "schema = \"\"\"\n",
    "{\n",
    "  \"title\": \"User\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"name\": {\"type\": \"string\"},\n",
    "    \"last_name\": {\"type\": \"string\"},\n",
    "    \"id\": {\"type\": \"integer\"},\n",
    "    \"height\": {\"type\": \"integer\"}\n",
    "  },\n",
    "  \"required\": [\"name\", \"last_name\", \"id\", \"height\"]\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)\n",
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "\n",
    "runner.add_schema(schema)\n",
    "responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
