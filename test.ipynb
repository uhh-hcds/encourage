{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# Set here the right device id!\n",
    "%env CUDA_VISIBLE_DEVICES=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from encourage.llm import BatchInferenceRunner\n",
    "from encourage.prompts.prompt_collection import PromptCollection\n",
    "from encourage.metrics.non_answer_critic import NonAnswerCritic\n",
    "from encourage.prompts.context import Context\n",
    "from encourage.prompts.meta_data import MetaData\n",
    "model_name= \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-03 16:36:40 arg_utils.py:930] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 12-03 16:36:40 config.py:1010] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 12-03 16:36:40 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-03 16:36:44 model_runner.py:1014] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...\n",
      "INFO 12-03 16:36:47 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbd6111e1924eb39e68067cb7b31e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 16:36:54 model_runner.py:1025] Loading model weights took 14.9888 GB\n",
      "INFO 12-03 16:36:55 gpu_executor.py:122] # GPU blocks: 11039, # CPU blocks: 2048\n",
      "INFO 12-03 16:36:59 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 12-03 16:36:59 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 12-03 16:37:29 model_runner.py:1456] Graph capturing finished in 31 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=model_name, gpu_memory_utilization=0.8)\n",
    "sampling_params = SamplingParams(temperature=0.5, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompts = \"You are an helpful AI that only speaks like a pirat\"\n",
    "user_prompts = [\"User prompt 1\", \"User prompt 2\"] * 5\n",
    "contexts = [Context.from_prompt_vars({\"key1\": \"value1\"}, {\"key2\": \"value2\"}) for _ in range(5)]\n",
    "meta_datas = [MetaData({\"meta\": \"data1\"}), MetaData({\"meta\": \"data2\"})] * 5\n",
    "\n",
    "prompt_collection = PromptCollection.create_prompts(\n",
    "    sys_prompts=sys_prompts,\n",
    "    user_prompts=user_prompts,\n",
    "    contexts=contexts,\n",
    "    meta_datas=meta_datas,\n",
    "    model_name=model_name,\n",
    "    template_name=\"llama3_custom.j2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encourage.llm.response import Response\n",
    "from encourage.llm import ResponseWrapper\n",
    "\n",
    "\n",
    "responses = ResponseWrapper([\n",
    "            Response(\n",
    "                request_id=\"1\",\n",
    "                prompt_id=\"p1\",\n",
    "                sys_prompt=\"System prompt example.\",\n",
    "                user_prompt=\"User prompt example.\",\n",
    "                response=\"This is a generated answer.\",\n",
    "                conversation_id=1,\n",
    "                meta_data={\n",
    "                    \"reference_answer\": \"This is the reference answer.\",\n",
    "                    \"reference_document\": [\"doc1\"],  # Required field for MRR\n",
    "                },\n",
    "                context={\n",
    "                    \"contexts\": [  # Required field for MRR\n",
    "                        {\"content\": \"Here is an example content\", \"document\": \"doc1\", \"score\": 1.0},\n",
    "                        {\"content\": \"Here is example content\", \"document\": \"doc2\", \"score\": 0.5},\n",
    "                    ]\n",
    "                },\n",
    "                arrival_time=0.0,\n",
    "                finished_time=1.0,\n",
    "            ),\n",
    "            Response(\n",
    "                request_id=\"2\",\n",
    "                prompt_id=\"p2\",\n",
    "                sys_prompt=\"Another system prompt.\",\n",
    "                user_prompt=\"Another user prompt.\",\n",
    "                response=\"Another generated answer.\",\n",
    "                conversation_id=2,\n",
    "                meta_data={\n",
    "                    \"reference_answer\": \"Another reference answer.\",\n",
    "                    \"reference_document\": [\"doc2\"],  # Required field for MRR\n",
    "                },\n",
    "                context={\n",
    "                    \"contexts\": [  # Required field for MRR\n",
    "                        {\"content\": \"Here is an example content\", \"document\": \"doc2\", \"score\": 1.0},\n",
    "                        {\"content\": \"Here is an example content\", \"document\": \"doc1\", \"score\": 0.0},\n",
    "                    ]\n",
    "                },\n",
    "                arrival_time=0.0,\n",
    "                finished_time=1.0,\n",
    "            ),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = ResponseWrapper([\n",
    "    Response(\n",
    "        request_id=\"0\",\n",
    "        prompt_id=\"p0\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"What is the highest peak in Jotunheimen National Park?\",\n",
    "        response=\"The highest peak in Jotunheimen National Park is Glittertind. It is 2,465 metres (8,087 ft) tall.\",\n",
    "        conversation_id=0,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"GaldhÃ¸piggen at 2,469 metres (8,100 ft).\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"More than 250 peaks rise above an elevation of 1,900 metres (6,200 ft), including Northern Europe's two highest peaks: GaldhÃ¸piggen at 2,469 metres (8,100 ft), and Glittertind at 2,465 metres (8,087 ft).\", \"document\": \"website1\", \"score\": 7},\n",
    "                {\"content\": \"Jotunheimen National Park is part of the larger area Jotunheimen.\", \"document\": \"website1\", \"score\": 6}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "    Response(\n",
    "        request_id=\"1\",\n",
    "        prompt_id=\"p1\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"Which counties is Jotunheimen National Park located in?\",\n",
    "        response=\"Jotunheimen National Park is located in Oslo and Akershus counties.\",\n",
    "        conversation_id=1,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"Innlandet and Vestland counties.\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"Jotunheimen includes Hurrungane, but Utladalen and its surroundings are within Utladalen Landscape Protection Area.\", \"document\": \"website2\", \"score\": 7},\n",
    "                {\"content\": \"Geographically, it lies in both Innlandet and Vestland counties.\", \"document\": \"website1\", \"score\": 5}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "    Response(\n",
    "        request_id=\"2\",\n",
    "        prompt_id=\"p2\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"What wildlife can be found in Jotunheimen National Park?\",\n",
    "        response=\"Wildlife in Jotunheimen National Park includes lynx, wolves, brown bears, and musk oxen.\",\n",
    "        conversation_id=2,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"Lynx, moose, Norwegian red deer, reindeer, roe deer, and wolverine.\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"Wildlife in the park include the lynx, moose, Norwegian red deer, reindeer, roe deer, and wolverine.\", \"document\": \"website1\", \"score\": 7},\n",
    "                {\"content\": \"Most lakes and rivers hold trout.\", \"document\": \"website2\", \"score\": 3}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "    Response(\n",
    "        request_id=\"3\",\n",
    "        prompt_id=\"p3\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"What archaeological discovery was made in Jotunheimen in 2020?\",\n",
    "        response=\"In 2020 a significant discovery was made in Jotunheimen by researchers. They found a 1,500-year-old Viking arrowhead, which was made of iron and locked away in a glacier.\",\n",
    "        conversation_id=3,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"A 1,500-year-old Viking arrowhead was discovered in Jotunheimen.\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"The arrowhead made of iron was revealed with its cracked wooden shaft and a feather, is 17 cm long and weighs just 28 grams.\", \"document\": \"website2\", \"score\": 6},\n",
    "                {\"content\": \"In February 2020, Secrets of the Ice Program researchers discovered a 1,500-year-old Viking arrowhead dating back to the Germanic Iron Age and locked in a glacier in southern Norway caused by the climate change in the Jotunheimen Mountains.\", \"document\": \"website1\", \"score\": 4}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "    Response(\n",
    "        request_id=\"4\",\n",
    "        prompt_id=\"p4\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"What did the Royal Road decree of the 15th century require?\",\n",
    "        response=\"The Royal Road decree of the 15th century required the residents of Lom to keep the mountain crossing passable to the middle of the Sognefjell.\",\n",
    "        conversation_id=4,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"The residents of Lom must keep the mountain crossing passable to the middle of the Sognefjell. This is so that traders coming from the north can reach their trading town Bergen.\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"A 'Royal Road' decree from the 15th century required that the residents of Lom must keep the mountain crossing passable to the middle of the Sognefjell, allowing traders to pass.\", \"document\": \"website1\", \"score\": 8},\n",
    "                {\"content\": \"Caravans carried farm products down the mountains and returned with salt, iron, cloth and lutefisk.\", \"document\": \"website1\", \"score\": 5}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "    Response(\n",
    "        request_id=\"5\",\n",
    "        prompt_id=\"p5\",\n",
    "        sys_prompt=\"System prompt example.\",\n",
    "        user_prompt=\"How many tourists hike Besseggen each year?\",\n",
    "        response=\"I'm sorry but I do not have any information about how many tourists come to Jotunheimen to hike the Besseggen ridge.\",\n",
    "        conversation_id=5,\n",
    "        meta_data={\n",
    "            \"reference_answer\": \"An estimated 60,000 people walk the Besseggen ridge every year.\",\n",
    "            \"reference_document\": [\"website1\"]\n",
    "        },\n",
    "        context={\n",
    "            \"contexts\": [\n",
    "                {\"content\": \"More than 250 peaks rise above an elevation of 1,900 metres (6,200 ft), including Northern Europe's two highest peaks: GaldhÃ¸piggen at 2,469 metres (8,100 ft), and Glittertind at 2,465 metres (8,087 ft).\", \"document\": \"website1\", \"score\": 7},\n",
    "                {\"content\": \"Jotunheimen National Park is part of the larger area Jotunheimen.\", \"document\": \"website1\", \"score\": 6}\n",
    "            ]\n",
    "        },\n",
    "        arrival_time=0.0,\n",
    "        finished_time=1.0\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What is the highest peak in Jotunheimen National Park?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The highest peak in Jotunheimen National Park is Glittertind. It is 2,465 metres (8,087 ft) tall.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'GaldhÃ¸piggen at 2,469 metres (8,100 ft).', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 0\n",
      "ðŸ†” Prompt ID: p0\n",
      "ðŸ†” Conversation ID: 0\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "Which counties is Jotunheimen National Park located in?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "Jotunheimen National Park is located in Oslo and Akershus counties.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'Innlandet and Vestland counties.', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 1\n",
      "ðŸ†” Prompt ID: p1\n",
      "ðŸ†” Conversation ID: 1\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What wildlife can be found in Jotunheimen National Park?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "Wildlife in Jotunheimen National Park includes lynx, wolves, brown bears, and musk oxen.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'Lynx, moose, Norwegian red deer, reindeer, roe deer, and wolverine.', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 2\n",
      "ðŸ†” Prompt ID: p2\n",
      "ðŸ†” Conversation ID: 2\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What archaeological discovery was made in Jotunheimen in 2020?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "In 2020 a significant discovery was made in Jotunheimen by researchers. They found a 1,500-year-old Viking arrowhead, which was made of iron and locked away in a glacier.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'A 1,500-year-old Viking arrowhead was discovered in Jotunheimen.', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 3\n",
      "ðŸ†” Prompt ID: p3\n",
      "ðŸ†” Conversation ID: 3\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "What did the Royal Road decree of the 15th century require?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "The Royal Road decree of the 15th century required the residents of Lom to keep the mountain crossing passable to the middle of the Sognefjell.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'The residents of Lom must keep the mountain crossing passable to the middle of the Sognefjell. This is so that traders coming from the north can reach their trading town Bergen.', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 4\n",
      "ðŸ†” Prompt ID: p4\n",
      "ðŸ†” Conversation ID: 4\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n",
      "--------------------------------------------------\n",
      "ðŸ§‘â€ðŸ’» User Prompt:\n",
      "How many tourists hike Besseggen each year?\n",
      "ðŸ“š Added Context keys: contexts (See Template for details.)\n",
      "\n",
      "ðŸ’¬ Response:\n",
      "I'm sorry but I do not have any information about how many tourists come to Jotunheimen to hike the Besseggen ridge.\n",
      "\n",
      "ðŸ¤– System Prompt:\n",
      "System prompt example.\n",
      "\n",
      "ðŸ—‚ï¸ Metadata: {'reference_answer': 'An estimated 60,000 people walk the Besseggen ridge every year.', 'reference_document': ['website1']}\n",
      "ðŸ†” Request ID: 5\n",
      "ðŸ†” Prompt ID: p5\n",
      "ðŸ†” Conversation ID: 5\n",
      "â³ Processing Time: 1.0 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner = BatchInferenceRunner(llm, sampling_params)\n",
    "# responses = runner.run(prompt_collection)\n",
    "responses.print_response_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.50s/it, est. speed input: 317.16 toks/s, output: 39.89 toks/s]\n",
      "Compiling FSM index for all state transitions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:01<00:00, 44.31it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.63it/s, est. speed input: 1234.91 toks/s, output: 46.82 toks/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from encourage.metrics.context_recall import ContextRecall\n",
    "from encourage.metrics.context_precision import ContextPrecision\n",
    "from encourage.metrics.answer_similarity import AnswerSimilarity\n",
    "from encourage.metrics.answer_relevance import AnswerRelevance\n",
    "from encourage.metrics.answer_faithfulness import AnswerFaithfulness\n",
    "\n",
    "# metric = ContextRecall(runner)\n",
    "# metric = AnswerSimilarity(\"all-mpnet-base-v2\")\n",
    "metric = AnswerFaithfulness(runner)\n",
    "\n",
    "test = metric(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simpler_statements'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ('simpler_statements', ['I do not have any information about how many tourists come to Jotunheimen to hike the Besseggen ridge.'])\n",
    "b,c = a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.0,\n",
       " 'raw': [1.0, nan, nan, nan, nan, nan],\n",
       " 'supported': [2, 0, 0, 0, 0, 0],\n",
       " 'total': [2, 0, 0, 0, 0, 0],\n",
       " 'claims': [[Verdict(statement='I do not have any information about tourists coming to Jotunheimen to hike the Besseggen ridge.', reason='The context does not mention anything about tourists coming to Jotunheniem to hike the Besseggen ridge.', verdict=1),\n",
       "   Verdict(statement='I do not have any information about the number of tourists coming to Jotunheimen to hike the Besseggen ridge.', reason='The context does not mention anything about tourists coming to Jotunheniem to hike the Besseggen ridge.', verdict=1)],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  [],\n",
       "  []]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MetricOutput' object has no attribute 'print_metric_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_metric_summary\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MetricOutput' object has no attribute 'print_metric_summary'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
